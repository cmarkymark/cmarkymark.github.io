---
title: "K-Cross Validation Random Trees"
author: "Charles Marks"
date: "December 8, 2018"
output: html_document
---

```{r global options}
library(tidyverse)
library(tableone)
library(PerformanceAnalytics)
library(psych)
library(ROCR)
library(randomForest)
options(digits = 2)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
## K-Cross Validation With Random Forest Model

So, like with the logistic regression model, we will assign the participants to 5 groups to do 80-20 cross validation and choose the best performing model.  For this doc, we have excluded the code for loading the data and preparing it for analysis.  See the kcrossval.rmd for that code.

## Creating the five groups for cross-validation

Generate a random vector with equal number of 1 - 5, and then assign this vector to the dataframe, in effect, assigning each row randomlly to one of five groups.

```{r fivegroups}
# this code will generate a random vector for the assignment of groups
values <- rep(1:5, length.out=391)
random <- sample(values)
clean_data$cv_group <- random

```

## Defining Our Five Groups

This code shall create dataframes for each of the five test and training sets.

```{r define groups}

train.one <- clean_data[which(clean_data$cv_group!= 1),]
test.one <- clean_data[which(clean_data$cv_group== 1),]

train.two <- clean_data[which(clean_data$cv_group!= 2),]
test.two <- clean_data[which(clean_data$cv_group== 2),]

train.three <- clean_data[which(clean_data$cv_group!= 3),]
test.three <- clean_data[which(clean_data$cv_group== 3),]

train.four <- clean_data[which(clean_data$cv_group!= 4),]
test.four <- clean_data[which(clean_data$cv_group== 4),]

train.five <- clean_data[which(clean_data$cv_group!= 5),]
test.five <- clean_data[which(clean_data$cv_group== 5),]

```

## Running Cross Val

The following 5 code sections run the five models, as well as plot the AUC plots and return the AUROC values, which shall be utilized to determine the model of best fit.

### Test 1

```{r test 1, include=FALSE}



model.rf.one = randomForest(any_glaucoma_surgey ~ Age + Gender +systolic_max + systolic_min + systolic_mean  + diastolic_max + diastolic_min + diastolic_mean +pulse_min + pulse_max + pulse_mean  +ever_hospitalized + days_hospitalized + CHF + PVD + Stroke + Dementia + Pulmonary + LiverMild + DM + DMcx  + Renal + Cancer + Mets + med_class.Analgesics_._non.opioids + med_class.Analgesics_._opioids  +  med_class.Anti.rheumatic +  med_class.Antianxiety_agents  + med_class.Antiasthmatic + med_class.Anticoagulants+med_class.Anticonvulsant+med_class.Antidepressants+med_class.Antidiabetic+med_class.Antiemetics+med_class.Antihyperlipidemic+med_class.Antihypertensive+med_class.Beta_blockers+med_class.Calcium_blockers+med_class.Corticosteroids+med_class.Cough.Cold+med_class.Decongestants+med_class.Dermatological+med_class.Diuretics+med_class.Laxatives+med_class.Macrolide_antibiotics+med_class.Misc._antiinfectives+med_class.Ophthalmic+med_class.Ulcer_drugs, 
                 data=train.one, importance = TRUE, mtry = 6
                 )

rf.one.pr <- predict(model.rf.one, test.one, type="prob")[,2]
rf.one.pred <- prediction(rf.one.pr,test.one$any_glaucoma_surgey)
rf.one.perf <- performance(rf.one.pred,"tpr","fpr")

plot(rf.one.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

rf.one.auc <- performance(rf.one.pred,"auc")@y.values[[1]]

```
### Additional Code Snippet -- Sensitivity, Specificity, Accuracy

The following code snippet is used to identify the best cutpoint and the associated senitivity, specificity, and accuracy of the model.

```{r get sensitivity and specificity}

# this function identifies the best cut point
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}
print(opt.cut(rf.one.perf, rf.one.pred))

# this measures the accuracy across the range of cutpoint and displays an accuracy chart
acc.perf = performance(rf.one.pred, measure = "acc")
plot(acc.perf)

ind = which.max( slot(acc.perf, "y.values")[[1]] )
acc = slot(acc.perf, "y.values")[[1]][ind]
cutoff = slot(acc.perf, "x.values")[[1]][ind]
print(c(accuracy= acc, cutoff = cutoff))

```

### Test 2

```{r test 2, include=FALSE}



model.rf.two = randomForest(any_glaucoma_surgey ~ Age + Gender +systolic_max + systolic_min + systolic_mean  + diastolic_max + diastolic_min + diastolic_mean +pulse_min + pulse_max + pulse_mean  +ever_hospitalized + days_hospitalized  +CHF + PVD + Stroke + Dementia + Pulmonary + LiverMild + DM + DMcx  + Renal + Cancer + Mets + med_class.Analgesics_._non.opioids + med_class.Analgesics_._opioids  +  med_class.Anti.rheumatic +  med_class.Antianxiety_agents  + med_class.Antiasthmatic + med_class.Anticoagulants+med_class.Anticonvulsant+med_class.Antidepressants+med_class.Antidiabetic+med_class.Antiemetics+med_class.Antihyperlipidemic+med_class.Antihypertensive+med_class.Beta_blockers+med_class.Calcium_blockers+med_class.Corticosteroids+med_class.Cough.Cold+med_class.Decongestants+med_class.Dermatological+med_class.Diuretics+med_class.Laxatives+med_class.Macrolide_antibiotics+med_class.Misc._antiinfectives+med_class.Ophthalmic+med_class.Ulcer_drugs, 
                 data=train.two, importance = TRUE, mtry = 6
                 )

rf.two.pr <- predict(model.rf.two, test.two, type="prob")[,2]
rf.two.pred <- prediction(rf.two.pr,test.two$any_glaucoma_surgey)
rf.two.perf <- performance(rf.two.pred,"tpr","fpr")

plot(rf.two.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

rf.two.auc <- performance(rf.two.pred,"auc")@y.values[[1]]

```

### Test 3

```{r test 3, include=FALSE}



model.rf.three = randomForest(any_glaucoma_surgey ~ Age + Gender +systolic_max + systolic_min + systolic_mean  + diastolic_max + diastolic_min + diastolic_mean +pulse_min + pulse_max + pulse_mean  +ever_hospitalized + days_hospitalized  +CHF + PVD + Stroke + Dementia + Pulmonary + LiverMild + DM + DMcx  + Renal + Cancer + Mets + med_class.Analgesics_._non.opioids + med_class.Analgesics_._opioids  +  med_class.Anti.rheumatic +  med_class.Antianxiety_agents  + med_class.Antiasthmatic + med_class.Anticoagulants+med_class.Anticonvulsant+med_class.Antidepressants+med_class.Antidiabetic+med_class.Antiemetics+med_class.Antihyperlipidemic+med_class.Antihypertensive+med_class.Beta_blockers+med_class.Calcium_blockers+med_class.Corticosteroids+med_class.Cough.Cold+med_class.Decongestants+med_class.Dermatological+med_class.Diuretics+med_class.Laxatives+med_class.Macrolide_antibiotics+med_class.Misc._antiinfectives+med_class.Ophthalmic+med_class.Ulcer_drugs, 
                 data=train.three, importance = TRUE, mtry = 6
                 )

rf.three.pr <- predict(model.rf.three, test.three, type="prob")[,2]
rf.three.pred <- prediction(rf.three.pr,test.three$any_glaucoma_surgey)
rf.three.perf <- performance(rf.three.pred,"tpr","fpr")

plot(rf.three.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

rf.three.auc <- performance(rf.three.pred,"auc")@y.values[[1]]

```
### Test 4

```{r test 4, include=FALSE}



model.rf.four = randomForest(any_glaucoma_surgey ~ Age + Gender +systolic_max + systolic_min + systolic_mean  + diastolic_max + diastolic_min + diastolic_mean +pulse_min + pulse_max + pulse_mean  +ever_hospitalized + days_hospitalized  +CHF + PVD + Stroke + Dementia + Pulmonary + LiverMild + DM + DMcx  + Renal + Cancer + Mets + med_class.Analgesics_._non.opioids + med_class.Analgesics_._opioids  +  med_class.Anti.rheumatic +  med_class.Antianxiety_agents  + med_class.Antiasthmatic + med_class.Anticoagulants+med_class.Anticonvulsant+med_class.Antidepressants+med_class.Antidiabetic+med_class.Antiemetics+med_class.Antihyperlipidemic+med_class.Antihypertensive+med_class.Beta_blockers+med_class.Calcium_blockers+med_class.Corticosteroids+med_class.Cough.Cold+med_class.Decongestants+med_class.Dermatological+med_class.Diuretics+med_class.Laxatives+med_class.Macrolide_antibiotics+med_class.Misc._antiinfectives+med_class.Ophthalmic+med_class.Ulcer_drugs, 
                 data=train.four, importance = TRUE, mtry = 6
                 )

rf.four.pr <- predict(model.rf.four, test.four, type="prob")[,2]
rf.four.pred <- prediction(rf.four.pr,test.four$any_glaucoma_surgey)
rf.four.perf <- performance(rf.four.pred,"tpr","fpr")

plot(rf.four.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

rf.four.auc <- performance(rf.four.pred,"auc")@y.values[[1]]

```
### Test 5

```{r test 5, include=FALSE}



model.rf.five = randomForest(any_glaucoma_surgey ~ Age + Gender +systolic_max + systolic_min + systolic_mean  + diastolic_max + diastolic_min + diastolic_mean +pulse_min + pulse_max + pulse_mean  +ever_hospitalized + days_hospitalized  +CHF + PVD + Stroke + Dementia + Pulmonary + LiverMild + DM + DMcx  + Renal + Cancer + Mets + med_class.Analgesics_._non.opioids + med_class.Analgesics_._opioids  +  med_class.Anti.rheumatic +  med_class.Antianxiety_agents  + med_class.Antiasthmatic + med_class.Anticoagulants+med_class.Anticonvulsant+med_class.Antidepressants+med_class.Antidiabetic+med_class.Antiemetics+med_class.Antihyperlipidemic+med_class.Antihypertensive+med_class.Beta_blockers+med_class.Calcium_blockers+med_class.Corticosteroids+med_class.Cough.Cold+med_class.Decongestants+med_class.Dermatological+med_class.Diuretics+med_class.Laxatives+med_class.Macrolide_antibiotics+med_class.Misc._antiinfectives+med_class.Ophthalmic+med_class.Ulcer_drugs, 
                 data=train.five, importance = TRUE, mtry = 6
                 )

rf.five.pr <- predict(model.rf.five, test.five, type="prob")[,2]
rf.five.pred <- prediction(rf.five.pr,test.five$any_glaucoma_surgey)
rf.five.perf <- performance(rf.five.pred,"tpr","fpr")

plot(rf.five.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

rf.five.auc <- performance(rf.five.pred,"auc")@y.values[[1]]

```
### COMAPRE ROC

The following code outputs the AUROC scores for each model, allowing us the determine the best fitting model.

```{r roc comapre}

print(rf.one.auc)
print(rf.two.auc)
print(rf.three.auc)
print(rf.four.auc)
print(rf.five.auc)

```

## Results of Best Model

We want to return the importance scores of the best performing model to understand which variables played the most significant role in this model.

```{r looking at the best}

results <- importance(model.rf.two)
results

```

## Evaluating This Model

### Discrimination (C-Statistic aka AUC)

```{r AUC}

plot(rf.two.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")


```

### Caibration (Hoslem-Lemeshow)

```{r calibration}


library(gbm)
calibrate.plot(test.two$any_glaucoma_surgey, rf.two.pr)
```


```{r Saving Docs}

final_test_set <- test.two
final_training_set <- train.two

importance_scores <- importance(model.rf.two)

write.csv(final_test_set, "Final_Test_Set_RandomForests_Dec10.csv")
write.csv(final_training_set, "Final_Training_Set_RandomForests_Dec8.csv")
write.csv(importance_scores, "Final_Importance_Scores_random_Forest.csv")

```
