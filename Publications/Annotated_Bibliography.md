### Annotated Bibliography

I wanted to create a space where I can reflect on papers.  While I am pretty early stage in this academia thing, I already sense I have lost quite a bit of the memorty of my first publications and it seems wise to create a living document of this type.
Now, I am not sure exactly if you will find this particularly useful, but I feel (currently starting this as a 3rd year doctoral student), that I don't really know what academic trajectories look like, even when I am surrounded by academics.
In fact, I feel, so far, I have presented myself as being far more knowledgable than I truly am and I think this also can be a good space ot articulate how messy a lot of this work felt in the moment (which I think is a privilege I can only undertake now with a few publications under my belt and the assumption that few people will take a gander at this page at all).

These are presented in chronological order for which they were accepted for publication, though not necessarily reflective of the actual publication date: 

**Marks, C.**, Zúñiga, M.L., Identifying Key Subpopulations Among HIV+ Latinos Receiving Care in San Diego-Tijuana & Assessing CAM Utilization and ARV Adherence: A Latent Class Analysis. Frontiers in Public Health [DOI: 10.3389/fpubh.2019.00179](https://doi.org/10.3389/fpubh.2019.00179)

I initially wrote this in the first year of my doctoral program for a multivariate statistics class.  The director of our program, Dr. Zuniga, had been sitting on the data for a while and was kind enough to allow us to use it for the class.  I have a background in computer science and math (not statistics), so this made adopting R fairly straightforward, though I admit my understanding of statistical concepts was rather limited.  I have noticed whenever I have learned something new (usually from making mistakes), it is then really easy to act like you're an expert (even when you just learned that thing like a few days prior).  Anyway, Dr. Zuniga's study focused on Complementary and Alternative Medicine and medication adherence among HIV+ Latinos in the border region.  The general idea was to use a latent class analysis to divide population based on membership in three high-risk HIV groups: people who inject; men who have sex with men; and those who have traded sex.  I believe this was actually the initial idea we landed upon and this analysis made it into the published paper without edit (I will continue to look, but I regret that I am unsure where I saved the R Code...I did the first year of my program using public computers and a chromebook....)  As was noted by another program director, this approach of dividing the population based on membership in these three groups might be seen as arbitrary, for we could have chosen other factors (such as income or nationality) which likely would've bore similar results, but I think the big takeaway I took from this work was that the explanation for your choice in methods are an integral part of the analysis as a whole.  This is social science statistics, there are inherent flaws with every analysis, it's not just our job to run the analysis, but to convince the reader the analysis is meaningful.

Baxter, S., **Marks, C.**, Kuo, T., Ohno-Machado, L., Weinreb, R., Machine learning-based predictive modeling of surgical intervention in glaucoma using systemic data from electronic health records. American Journal of Ophtalmology. 2019 In Press [DOI: 10.1016/j.ajo.2019.07.005](https://doi.org/10.1016/j.ajo.2019.07.005)

In my second year of my PhD program I took a bioinformatics course and was very very very fortunate to be paired up with Dr. Baxter -- I feel it is really important to articulate the role that luck plays in this whole process (regardless of the quality of work or amount of sweat that went into it), because when we go into job interviews and such we aren't exactly supposed to be like "well I got real lucky this one time...".  As a result, I think I can now claim to be a data scientist -- I handled the analytic portion of this paper.  In this paper, the question was, can basic EHR record information (hospital visits, medications, and general vitals) predict glaucoma risk.  We employed three learning models (log regression, random forest, aritifical neural nets) and used a leave-one out cross validation to assess predictive performance.  We initially used a 5-fold cross-validation, but given the small sample size (around 400), the reviewers were like "...do the LOO, bro."  This was also my first forray into major data cleaning and writing replicable code.  It is nice though, because every time I wanna do a cross-validation now I just copy and paste the code and just do a little bit of editing.

**Marks C.**, Jahangiri, A.,Machiani, S.G.,Iterative DBSCAN (I-DBSCAN) to Identify Aggressive Driving Behaviors within Unlabeled Real-World Driving Data IEEE ITSC 2019, New Zealand 

After my first year in the program, I took a side job (one of many) with the engineering department at SDSU trying to identify aggressive driving from real world driving data -- this has pretty much nothing to do with what I study.  About halfway through, in some ways, I think it became apparent that we needed some tangibles, and so I came up with this algorithm (which one could argue is more of an application of an old algorithm (i mean...that's basically what most science is)).  I dedicated a huge chunk of my time (and emotional energy) on this project, in large part because I  was paid pretty well and because, well, my contributions were frequently validated and I am a human with imposter syndrome just like the next aspiring academic. 

**Marks, C.**, Borquez, A., Jain, S., Sun, X., Strathdee, S., Garfein, R., Milloy, M-J, DeBeck, K., Cepeda, J., Werb, D., Martin, N., Opioid agonist treatment scale-up and the initiation of injection drug use: a dynamic modeling analysis. PLOS Medicine. 2019 [DOI: 10.1371/journal.pmed.1002973](https://doi.org/10.1371/journal.pmed.1002973)   

Early in my program I was paired with Drs Werb and Martin and Dr. Borquez because I "knew math" -- please return to my prior comment about the role that luck plays in all of this.  The general idea was that we wanted to model how scaling up opioid agonist treatment among PWID would impact the number of PWID who initiate.  The first year of this project was a mix of meetings and assignments both to conceptualize what this model might look like, but also for me to prove I could handle the project.  We had a goal of submitting to a special issue of IJDP at the end of the summer 2019, but when we circulated an early draft to co-authors, one co-author said, "well why not submit to PLOS Medicine by early summer" which then began the most hectic 6 month of work I've ever undertaken.  Given the additional statistical rigor of PLOS Med, we turned around a new shiny draft within about 45 days.  Basically two days later I began my two week competency exams and then we got back the feedback indicating acceptance upon several revisions.  While straightforward, we were given two weeks and by the end of these two weeks (including one particularly horrible night where I thought that I had an error in my code that rendered the whole paper broken -- just exhausted paranoia though).  By the end of this I was a puddle of mush.  I wept when we got the official acceptance letter.  As of writing this it is kind of out of site, out of mind.  My advisors told me "PLOS Medicine is a big deal" but, quite honestly, I am a new researcher in the Google Scholar era, I tend to just make sure research is in a real journal, but besides that I give it very little mind.  So I will return back to this if I ever get tangible proof that it is, in fact, "a big deal"







